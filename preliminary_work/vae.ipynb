{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import notebook, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/preproc-vae-notebook/valid_sq.pickle\n",
      "/kaggle/input/preproc-vae-notebook/custom.css\n",
      "/kaggle/input/preproc-vae-notebook/__notebook__.ipynb\n",
      "/kaggle/input/preproc-vae-notebook/__results__.html\n",
      "/kaggle/input/preproc-vae-notebook/__output__.json\n",
      "/kaggle/input/preproc-vae-notebook/train_sq.pickle\n",
      "/kaggle/input/preproc-vae-notebook/test_sq.pickle\n"
     ]
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/kaggle/input/preproc-vae-notebook/train_sq.pickle', 'rb') as train_pickle:\n",
    "    train_data = 127 - pickle.load(train_pickle)\n",
    "    \n",
    "with open('/kaggle/input/preproc-vae-notebook/valid_sq.pickle', 'rb') as valid_pickle:\n",
    "    valid_data = 127 - pickle.load(valid_pickle)\n",
    "\n",
    "with open('/kaggle/input/preproc-vae-notebook/test_sq.pickle', 'rb') as test_pickle:\n",
    "    test_data = 127 - pickle.load(test_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class MultiResBlock(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super().__init__()\n",
    "        self.conv1x1 = nn.Conv2d(ch_in, ch_out, kernel_size=1, padding=0)\n",
    "        self.bnorm1x1 = nn.BatchNorm2d(ch_out,track_running_stats=False)\n",
    "        self.fconv = nn.Conv2d(ch_in, ch_out//6, kernel_size=3, padding=1)\n",
    "        self.fbnorm = nn.BatchNorm2d(ch_out//6,track_running_stats=False)\n",
    "        self.sconv = nn.Conv2d(ch_out//6, ch_out//3, kernel_size=3, padding=1)\n",
    "        self.sbnorm = nn.BatchNorm2d(ch_out//3,track_running_stats=False)\n",
    "        self.tconv = nn.Conv2d(ch_out//3, ch_out//2+1, kernel_size=3, padding=1)\n",
    "        self.tbnorm = nn.BatchNorm2d(ch_out//2+1,track_running_stats=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        res1x1 = self.sigmoid(self.bnorm1x1(self.conv1x1(x)))\n",
    "        #print(\"res1x1 done\")\n",
    "        first = self.sigmoid(self.fbnorm(self.fconv(x)))\n",
    "        #print(\"fconv done\")\n",
    "        second = self.sigmoid(self.sbnorm(self.sconv(first)))\n",
    "        third = self.sigmoid(self.tbnorm(self.tconv(second)))\n",
    "        resconv = torch.cat((first,second,third),dim=1)\n",
    "        y = res1x1+resconv\n",
    "        return y\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, num_neurons, num_neurons2, num_neurons3, kernel_size, pool_size, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mrb1 = MultiResBlock(1, num_neurons)\n",
    "        self.mrb2 = MultiResBlock(num_neurons, num_neurons2)\n",
    "        self.mrb3 = MultiResBlock(num_neurons2, num_neurons3)\n",
    "        self.mrb3b = MultiResBlock(num_neurons3, num_neurons3)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.up = nn.Upsample(scale_factor=2)\n",
    "\n",
    "        self.mrb4 = MultiResBlock(num_neurons3, num_neurons2)\n",
    "        self.mrb5 = MultiResBlock(num_neurons2, num_neurons)\n",
    "        self.mrb6 = MultiResBlock(num_neurons, num_neurons)\n",
    "        self.mrb7 = MultiResBlock(num_neurons, num_neurons)\n",
    "        self.conv1x1 = nn.Conv2d(num_neurons, 1, kernel_size=3, padding=1)\n",
    "        # Fully connected layers\n",
    "        self.fc_mean = nn.Linear(128*16*16, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128*16*16, latent_dim)\n",
    "\n",
    "        self.fc_decoder = nn.Linear(latent_dim, 128*16*16)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def encoder(self, x):\n",
    "        x = self.mrb1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.mrb2(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.mrb3(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.mrb3b(x)\n",
    "        x = self.maxpool(x)\n",
    "        return x\n",
    "    \n",
    "    def decoder(self, x):\n",
    "        x = self.mrb4(x)\n",
    "        x = self.up(x)\n",
    "        x = self.mrb5(x)\n",
    "        x = self.up(x)\n",
    "        x = self.mrb6(x)\n",
    "        x = self.up(x)\n",
    "        x = self.mrb7(x)\n",
    "        x = self.up(x)\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def latent(self, z_mu, z_logvar):\n",
    "        ''' \n",
    "            encoder: z = mu + sd * e\n",
    "            input: mean, logvar. output: z\n",
    "        '''\n",
    "        sd = torch.exp(z_logvar * 0.5)\n",
    "        e = Variable(torch.randn(sd.size()).cuda())\n",
    "        z = e.mul(sd).add_(z_mu)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x).view(-1, 16*16*128)\n",
    "        z_mean = self.fc_mean(x)\n",
    "        z_logvar = self.fc_logvar(x)\n",
    "        \n",
    "        z = self.latent(z_mean, z_logvar)\n",
    "        z_decoder = self.fc_decoder(z).view(-1, 128, 16, 16)\n",
    "        x_out = self.decoder(z_decoder)\n",
    "        #print(\"x out shape: \", x_out.shape)\n",
    "        return x_out, z_mean, z_logvar\n",
    "\n",
    "\n",
    "def criterion(x_out, target, z_mean, z_logvar, alpha=0.5, beta=0.5):\n",
    "    \"\"\"\n",
    "    Criterion for VAE done analytically\n",
    "    output: loss\n",
    "    output: mse\n",
    "    output: KL Divergence\n",
    "    \"\"\"\n",
    "    mse = F.mse_loss(x_out, target, reduction='sum') #Use MSE loss for images\n",
    "    kl = -0.5 * torch.sum(1 + z_logvar - (z_mean**2) - torch.exp(z_logvar)) #Analytical KL Divergence - Assumes p(z) is Gaussian Distribution\n",
    "    loss = ((alpha * mse) + (beta * kl)) / x_out.size(0)\n",
    "    return loss, mse, kl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def batch_generator(data, batch_size=1, shuffle=False):\n",
    "    nsamples = len(data)\n",
    "    if shuffle:\n",
    "        perm = np.random.permutation(nsamples)\n",
    "    else:\n",
    "        perm = range(nsamples)\n",
    "\n",
    "    for i in range(0, nsamples, batch_size):\n",
    "        batch_idx = perm[i:i+batch_size]\n",
    "        yield data[batch_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Model Parameters ################\n",
    "\n",
    "num_neurons = 32\n",
    "num_neurons2 = 64\n",
    "num_neurons3 = 128\n",
    "kernel_size = 3\n",
    "pool_size = 2\n",
    "epochs = 25\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Additional functions ###############\n",
    "\n",
    "def train(model, optimizer, data, batch_size, epoch, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    losses = []\n",
    "    mses = []\n",
    "    kls = []\n",
    "    for X in batch_generator(data, batch_size, shuffle=True):\n",
    "        X = torch.from_numpy(X).float().to(device)\n",
    "        X = X/127\n",
    "        X = torch.unsqueeze(X,1)\n",
    "        model.zero_grad()\n",
    "        output, z_mu, z_logvar = model(X)\n",
    "        #print(\"Output size: \", output.shape)\n",
    "        #print(\"Forward pass\")\n",
    "        loss, mse, kl = criterion(output, X, z_mu, z_logvar)\n",
    "        #print(\"Loss computed\")\n",
    "        loss.backward()\n",
    "        #print(\"Backward pass done\")\n",
    "        optimizer.step()\n",
    "        #print(\"Optimezer step done\")\n",
    "        # Training statistics\n",
    "        #print(loss.item())\n",
    "        total_loss += loss.item()\n",
    "        losses.append(loss.item())\n",
    "        mses.append(mse.item())\n",
    "        kls.append(kl.item())\n",
    "\n",
    "    print(f'train loss={np.mean(losses):.6f} | mse loss:{np.mean(mses):.6f} | kl loss:{np.mean(kls):.6f}')\n",
    "    return total_loss\n",
    "\n",
    "def validate(model, data, batch_size, epoch, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    losses = []\n",
    "    mses = []\n",
    "    kls = []\n",
    "    with torch.no_grad():\n",
    "        for X in batch_generator(data, batch_size, shuffle=True):\n",
    "            X = torch.from_numpy(X).float().to(device)\n",
    "            X = X/127\n",
    "            X = torch.unsqueeze(X,1)\n",
    "            output, z_mu, z_logvar = model(X)\n",
    "            #print(\"Output size: \", output.shape)\n",
    "            #print(\"Forward pass\")\n",
    "            loss, mse, kl = criterion(output, X, z_mu, z_logvar)\n",
    "            total_loss += loss.item()\n",
    "            losses.append(loss.item())\n",
    "            mses.append(mse.item())\n",
    "            kls.append(kl.item())\n",
    "        print(f'| epoch {epoch:03d} | validation loss={np.mean(losses):.6f} | validation mse loss:{np.mean(mses):.6f} | validation kl loss:{np.mean(kls):.6f}')\n",
    "        return total_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss=2472.915163 | mse loss:309312.019567 | kl loss:7158.791315\n",
      "| epoch 000 | validation loss=295.902927 | validation mse loss:36267.954532 | validation kl loss:609.253700\n",
      "train loss=204.579540 | mse loss:25802.019926 | kl loss:352.326644\n",
      "| epoch 001 | validation loss=159.955256 | validation mse loss:19762.213472 | validation kl loss:186.696231\n",
      "train loss=139.399070 | mse loss:17666.706411 | kl loss:149.188806\n",
      "| epoch 002 | validation loss=130.741865 | validation mse loss:16149.155090 | validation kl loss:115.361711\n",
      "train loss=120.926655 | mse loss:15342.558492 | kl loss:112.658813\n",
      "| epoch 003 | validation loss=120.356443 | validation mse loss:14815.017930 | validation kl loss:47.934926\n",
      "train loss=114.235430 | mse loss:14324.380886 | kl loss:272.720163\n",
      "| epoch 004 | validation loss=114.059134 | validation mse loss:14135.351322 | validation kl loss:45.883880\n",
      "train loss=113.454105 | mse loss:13790.643879 | kl loss:708.730565\n",
      "| epoch 005 | validation loss=112.798089 | validation mse loss:13772.693503 | validation kl loss:193.086166\n",
      "train loss=111.636821 | mse loss:13477.808183 | kl loss:785.864440\n",
      "| epoch 006 | validation loss=128.241620 | validation mse loss:13452.385315 | validation kl loss:2479.099488\n",
      "train loss=113.657314 | mse loss:13269.952512 | kl loss:1253.964977\n",
      "| epoch 007 | validation loss=122.076815 | validation mse loss:13313.365001 | validation kl loss:1874.004311\n",
      "train loss=111.317321 | mse loss:13124.860484 | kl loss:1095.685577\n",
      "| epoch 008 | validation loss=142.397614 | validation mse loss:13190.880224 | validation kl loss:4558.233415\n",
      "train loss=113.166649 | mse loss:13024.701958 | kl loss:1439.637263\n",
      "| epoch 009 | validation loss=107.081299 | validation mse loss:13104.771657 | validation kl loss:128.977017\n",
      "train loss=105.974596 | mse loss:12951.681447 | kl loss:589.360079\n",
      "| epoch 010 | validation loss=117.280690 | validation mse loss:13057.164425 | validation kl loss:1621.364289\n",
      "train loss=103.586348 | mse loss:12898.414644 | kl loss:339.300025\n",
      "| epoch 011 | validation loss=106.884979 | validation mse loss:13007.414522 | validation kl loss:204.688867\n",
      "train loss=101.759991 | mse loss:12857.541962 | kl loss:144.050114\n",
      "| epoch 012 | validation loss=105.271924 | validation mse loss:12977.879384 | validation kl loss:65.396346\n",
      "train loss=101.214636 | mse loss:12825.643054 | kl loss:105.697365\n",
      "| epoch 013 | validation loss=105.554728 | validation mse loss:12950.394201 | validation kl loss:179.449606\n",
      "train loss=102.087273 | mse loss:12799.830651 | kl loss:247.555201\n",
      "| epoch 014 | validation loss=104.214131 | validation mse loss:12929.352374 | validation kl loss:94.477219\n",
      "train loss=102.261973 | mse loss:12780.827411 | kl loss:288.285426\n",
      "| epoch 015 | validation loss=104.819500 | validation mse loss:12912.404024 | validation kl loss:158.625384\n",
      "train loss=109.677108 | mse loss:12762.332549 | kl loss:1255.639408\n",
      "| epoch 016 | validation loss=103.818681 | validation mse loss:12898.669349 | validation kl loss:126.299947\n",
      "train loss=107.709999 | mse loss:12749.590857 | kl loss:1015.030213\n",
      "| epoch 017 | validation loss=104.536584 | validation mse loss:12882.132048 | validation kl loss:104.209526\n",
      "train loss=100.642502 | mse loss:12735.969222 | kl loss:122.105772\n",
      "| epoch 018 | validation loss=110.773294 | validation mse loss:12879.213749 | validation kl loss:951.380049\n",
      "train loss=107.076587 | mse loss:12727.444678 | kl loss:957.763834\n",
      "| epoch 019 | validation loss=103.649457 | validation mse loss:12865.379667 | validation kl loss:69.969775\n",
      "train loss=104.150255 | mse loss:12718.926571 | kl loss:588.876315\n",
      "| epoch 020 | validation loss=114.603101 | validation mse loss:12857.795608 | validation kl loss:1321.765757\n",
      "train loss=102.912488 | mse loss:12710.046866 | kl loss:439.556753\n",
      "| epoch 021 | validation loss=103.538556 | validation mse loss:12846.822172 | validation kl loss:78.966853\n",
      "train loss=102.944294 | mse loss:12703.963257 | kl loss:450.791922\n",
      "| epoch 022 | validation loss=104.038334 | validation mse loss:12836.032995 | validation kl loss:84.302939\n",
      "train loss=100.165960 | mse loss:12697.013973 | kl loss:104.772888\n",
      "| epoch 023 | validation loss=104.365437 | validation mse loss:12839.155974 | validation kl loss:122.595948\n",
      "train loss=105.253113 | mse loss:12690.528482 | kl loss:758.527809\n",
      "| epoch 024 | validation loss=103.639536 | validation mse loss:12830.947872 | validation kl loss:106.937615\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#file\n",
    "model = VAE(num_neurons, num_neurons2, num_neurons3, kernel_size, pool_size, latent_dim=256)\n",
    "\n",
    "#if torch.cuda.device_count() > 1:\n",
    "#    model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "#model = VAE(num_neurons, num_neurons2, num_neurons3, kernel_size, pool_size, latent_dim=128).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "#train_loss = []\n",
    "tr_loss = 0\n",
    "valid_loss = 0\n",
    "for epoch in range(epochs):\n",
    "    tr_loss = train(model, optimizer, train_data, batch_size=batch_size, epoch=epoch, device=device)\n",
    "    val_loss = validate(model, valid_data, batch_size, epoch, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(modeL, 'VAE_lt256_lr1e-4_inv.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}