{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-27T12:20:43.948987Z","start_time":"2018-11-27T12:20:30.225783Z"},"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport os\nimport sys\nimport random\nsys.path.append('/kaggle/input/midiutils/midi/')\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.utils.data as data","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-27T12:20:52.800756Z","start_time":"2018-11-27T12:20:43.965495Z"},"trusted":true},"cell_type":"code","source":"from midi_utils import midiread, midiwrite\nfrom matplotlib import pyplot as plt\nimport skimage.io as io\nfrom IPython.display import FileLink\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DataLoader"},{"metadata":{"ExecuteTime":{"end_time":"2018-11-27T12:20:52.867674Z","start_time":"2018-11-27T12:20:52.819795Z"},"trusted":true},"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.utils.data as data\n\n\ndef midi_filename_to_piano_roll(midi_filename):\n    \n    midi_data = midiread(midi_filename, dt=0.3)\n    \n    piano_roll = midi_data.piano_roll.transpose()\n    \n    # Pressed notes are replaced by 1\n    piano_roll[piano_roll > 0] = 1\n    \n    return piano_roll\n\n\ndef pad_piano_roll(piano_roll, max_length=132333, pad_value=0):\n        \n    original_piano_roll_length = piano_roll.shape[1]\n    \n    padded_piano_roll = np.zeros((88, max_length))\n    padded_piano_roll[:] = pad_value\n    \n    padded_piano_roll[:, -original_piano_roll_length:] = piano_roll\n\n    return padded_piano_roll\n\n\nclass NotesGenerationDataset(data.Dataset):\n    \n    def __init__(self, midi_folder_path, longest_sequence_length=1491):\n        \n        self.midi_folder_path = midi_folder_path\n        \n        midi_filenames = os.listdir(midi_folder_path)\n        \n        self.longest_sequence_length = longest_sequence_length\n        \n        midi_full_filenames = map(lambda filename: os.path.join(midi_folder_path, filename),midi_filenames)\n        \n        self.midi_full_filenames = list(midi_full_filenames)\n        \n        if longest_sequence_length is None:\n            \n            self.update_the_max_length()\n    \n    \n    def update_the_max_length(self):\n        \n        sequences_lengths = map(lambda filename: midi_filename_to_piano_roll(filename).shape[1],self.midi_full_filenames)\n        \n        max_length = max(sequences_lengths)\n        \n        self.longest_sequence_length = max_length\n                \n    \n    def __len__(self):\n        \n        return len(self.midi_full_filenames)\n    \n    def __getitem__(self, index):\n        \n        midi_full_filename = self.midi_full_filenames[index]\n        \n        piano_roll = midi_filename_to_piano_roll(midi_full_filename)\n        \n        # Shifting by one time step\n        sequence_length = piano_roll.shape[1] - 1\n        \n        # Shifting by one time step\n        input_sequence = piano_roll[:, :-1]\n        ground_truth_sequence = piano_roll[:, 1:]\n                \n        # padding sequence so that all of them have the same length\n        input_sequence_padded = pad_piano_roll(input_sequence, max_length=self.longest_sequence_length)\n        \n        ground_truth_sequence_padded = pad_piano_roll(ground_truth_sequence,max_length=self.longest_sequence_length,pad_value=-100)\n                \n        input_sequence_padded = input_sequence_padded.transpose()\n        ground_truth_sequence_padded = ground_truth_sequence_padded.transpose()\n        \n        return (torch.FloatTensor(input_sequence_padded),torch.LongTensor(ground_truth_sequence_padded),torch.LongTensor([sequence_length]) )\n\n    \ndef post_process_sequence_batch(batch_tuple):\n    \n    input_sequences, output_sequences, lengths = batch_tuple\n    \n    splitted_input_sequence_batch = input_sequences.split(split_size=1)\n    #print(splitted_input_sequence_batch[2].shape)\n    splitted_output_sequence_batch = output_sequences.split(split_size=1)\n    splitted_lengths_batch = lengths.split(split_size=1)\n\n    training_data_tuples = zip(splitted_input_sequence_batch,\n                               splitted_output_sequence_batch,\n                               splitted_lengths_batch)\n\n    training_data_tuples_sorted = sorted(training_data_tuples,\n                                         key=lambda p: int(p[2]),\n                                         reverse=True)\n\n    splitted_input_sequence_batch, splitted_output_sequence_batch, splitted_lengths_batch = zip(*training_data_tuples_sorted)\n\n    input_sequence_batch_sorted = torch.cat(splitted_input_sequence_batch)\n    output_sequence_batch_sorted = torch.cat(splitted_output_sequence_batch)\n    lengths_batch_sorted = torch.cat(splitted_lengths_batch)\n    input_sequence_batch_sorted = input_sequence_batch_sorted[:, -lengths_batch_sorted[0, 0]:, :]\n    output_sequence_batch_sorted = output_sequence_batch_sorted[:, -lengths_batch_sorted[0, 0]:, :]\n    \n    input_sequence_batch_transposed = input_sequence_batch_sorted.transpose(0, 1)\n    \n    lengths_batch_sorted_list = list(lengths_batch_sorted)\n    lengths_batch_sorted_list = map(lambda x: int(x), lengths_batch_sorted_list)\n    \n    return input_sequence_batch_transposed, output_sequence_batch_sorted, list(lengths_batch_sorted_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nseed = 1111\nrandom.seed(seed)\nnp.random.RandomState(seed)\ntorch.manual_seed(seed)\n\nif not torch.cuda.is_available():\n    print(\"WARNING: CUDA is not available. Select 'GPU On' on kernel settings\")\ndevice = torch.device(\"cuda\")\ntorch.cuda.manual_seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-27T12:21:07.678124Z","start_time":"2018-11-27T12:20:56.931002Z"},"trusted":true},"cell_type":"code","source":"trainset = NotesGenerationDataset('/kaggle/input/maestro/train/', longest_sequence_length=None)\n\ntrainset_loader = data.DataLoader(trainset, batch_size=8,shuffle=True, drop_last=True)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-27T12:21:08.232332Z","start_time":"2018-11-27T12:21:07.693745Z"},"trusted":true},"cell_type":"code","source":"X = next(iter(trainset_loader))\nX[0].shape","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-27T12:21:11.258476Z","start_time":"2018-11-27T12:21:08.259258Z"},"trusted":true},"cell_type":"code","source":"valset = NotesGenerationDataset('/kaggle/input/maestro/validation/', longest_sequence_length=None)\n\nvalset_loader = data.DataLoader(valset, batch_size=8, shuffle=False, drop_last=False)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-27T12:21:11.406325Z","start_time":"2018-11-27T12:21:11.278687Z"},"trusted":true},"cell_type":"code","source":"X_val = next(iter(valset_loader))\nX_val[0].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RNN"},{"metadata":{"ExecuteTime":{"end_time":"2018-11-27T12:22:33.314323Z","start_time":"2018-11-27T12:22:33.291386Z"},"trusted":true},"cell_type":"code","source":"class RNN(nn.Module):\n    \n    def __init__(self, input_size, hidden_size, num_classes, n_layers=2):\n        \n        super(RNN, self).__init__()\n        \n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_classes = num_classes\n        self.n_layers = n_layers\n        \n        self.notes_encoder = nn.Linear(in_features=input_size, out_features=hidden_size)\n        \n        self.bn = nn.BatchNorm1d(hidden_size)\n        \n        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers)\n        \n        self.logits_fc = nn.Linear(hidden_size, num_classes)\n    \n    \n    def forward(self, input_sequences, input_sequences_lengths, hidden=None):\n        batch_size = input_sequences.shape[1]\n\n        notes_encoded = self.notes_encoder(input_sequences)\n        \n        notes_encoded_rolled = notes_encoded.permute(1,2,0).contiguous()\n        notes_encoded_norm = self.bn(notes_encoded_rolled)\n        \n        notes_encoded_norm_drop = nn.Dropout(0.25)(notes_encoded_norm)\n        notes_encoded_complete = notes_encoded_norm_drop.permute(2,0,1)\n        \n        # Here we run rnns only on non-padded regions of the batch\n        packed = torch.nn.utils.rnn.pack_padded_sequence(notes_encoded_complete, input_sequences_lengths)\n        outputs, hidden = self.lstm(packed, hidden)\n        \n        # Here we unpack sequence(back to padded)\n        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n        \n        outputs_norm = self.bn(outputs.permute(1,2,0).contiguous())\n        outputs_drop = nn.Dropout(0.1)(outputs_norm)\n        logits = self.logits_fc(outputs_drop.permute(2,0,1))\n        logits = logits.transpose(0, 1).contiguous()\n        \n        neg_logits = (1 - logits)\n        \n        # Since the BCE loss doesn't support masking,crossentropy is used\n        binary_logits = torch.stack((logits, neg_logits), dim=3).contiguous()\n        logits_flatten = binary_logits.view(-1, 2)\n        return logits_flatten, hidden","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-27T12:22:57.954631Z","start_time":"2018-11-27T12:22:36.786295Z"},"trusted":true},"cell_type":"code","source":"model = RNN(input_size=88, hidden_size=512, num_classes=88).cuda()\n\ncriterion = nn.CrossEntropyLoss().cuda()\ncriterion_val = nn.CrossEntropyLoss().cuda()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-27T12:22:58.002722Z","start_time":"2018-11-27T12:22:57.987764Z"},"trusted":true},"cell_type":"code","source":"def validate(model):\n    model.eval()\n    full_val_loss = 0.0\n    overall_sequence_length = 0.0\n\n    for batch in valset_loader:\n\n        post_processed_batch_tuple = post_process_sequence_batch(batch)\n\n        input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n\n\n        output_sequences_batch_var =  Variable( output_sequences_batch.contiguous().view(-1).cuda() )\n\n        input_sequences_batch_var = Variable( input_sequences_batch.cuda() )\n\n        logits, _ = model(input_sequences_batch_var, sequences_lengths)\n\n        loss = criterion_val(logits, output_sequences_batch_var)\n\n        full_val_loss += loss.item()\n        overall_sequence_length += sum(sequences_lengths)\n\n    return full_val_loss / (overall_sequence_length * 88)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-27T12:23:12.210378Z","start_time":"2018-11-27T12:22:58.040622Z"},"trusted":true},"cell_type":"code","source":"validate(model)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-27T12:23:12.255258Z","start_time":"2018-11-27T12:23:12.244288Z"},"trusted":true},"cell_type":"code","source":"clip = 1.0\nepochs_number = 10\nsample_history = []\nbest_val_loss = float(\"inf\")","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-27T12:23:18.442825Z","start_time":"2018-11-27T12:23:18.43086Z"},"trusted":true},"cell_type":"code","source":"def lrfinder(start, end, model, trainset_loader, epochs=2):\n    model.train() # into training mode\n    lrs = np.linspace(start, end, epochs*len(trainset_loader))\n    parameters = filter(lambda p: p.requires_grad, model.parameters()) # get all parameters which need grad\n    optimizer = torch.optim.Adam(rnn.parameters(),start)\n    loss_list = []\n    ctr = 0\n    \n    for epoch_number in range(epochs):\n        epoch_loss = []\n        for batch in trainset_loader:\n            optimizer.param_groups[0]['lr'] = lrs[ctr]\n            ctr = ctr+1\n\n            post_processed_batch_tuple = post_process_sequence_batch(batch)\n\n            input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n\n            output_sequences_batch_var =  Variable( output_sequences_batch.contiguous().view(-1).cuda() )\n\n            input_sequences_batch_var = Variable( input_sequences_batch.cuda() )\n\n            optimizer.zero_grad()\n\n            logits, _ = model(input_sequences_batch_var, sequences_lengths)\n\n            loss = criterion(logits, output_sequences_batch_var)\n            loss_list.append(loss.item())\n            loss.backward()\n\n            torch.nn.utils.clip_grad_norm_(rnn.parameters(), clip)\n\n            optimizer.step()\n        print('Epoch %d' % epoch_number)\n    plt.plot(lrs, loss_list)\n    return lrs, loss_list","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-26T16:13:45.182544Z","start_time":"2018-11-26T16:11:13.582351Z"},"trusted":true},"cell_type":"code","source":"rnn = RNN(input_size=88, hidden_size=512, num_classes=88)\nrnn = rnn.cuda()\nlrs, losses = lrfinder(1e-4, 1e-1*5, rnn, trainset_loader)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-26T16:16:14.159868Z","start_time":"2018-11-26T16:16:13.950619Z"},"trusted":true},"cell_type":"code","source":"plt.plot(lrs[:15], losses[:15])","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-26T16:16:15.631943Z","start_time":"2018-11-26T16:16:15.43483Z"},"trusted":true},"cell_type":"code","source":"def get_triangular_lr(lr_low, lr_high, mini_batches):\n    iterations = mini_batches\n    lr_mid = lr_high/7 + lr_low\n    up = np.linspace(lr_low, lr_high, int(round(iterations*0.35)))\n    down = np.linspace(lr_high, lr_mid, int(round(iterations*0.35)))\n    floor = np.linspace(lr_mid, lr_low, int(round(iterations*0.30)))\n    return np.hstack([up, down[1:], floor])\n\nlrs_triangular = get_triangular_lr(1e-2, 1e-2*3.5, len(trainset_loader))\nplt.plot(lrs_triangular)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-26T16:16:17.208992Z","start_time":"2018-11-26T16:16:17.204006Z"},"trusted":true},"cell_type":"code","source":"clip = 1.0","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-26T16:16:18.949227Z","start_time":"2018-11-26T16:16:18.930281Z"},"trusted":true},"cell_type":"code","source":"def train_model(model, lrs_triangular, epochs_number=2, wd=0.0, best_val_loss=float(\"inf\")):\n    loss_list = []\n    val_list =[]\n    optimizer = torch.optim.Adam(rnn.parameters(), lr=lrs_triangular[0], weight_decay=wd)\n    for epoch_number in range(epochs_number):\n        model.train()\n        epoch_loss = []\n        for lr, batch in zip(lrs_triangular, trainset_loader):\n            optimizer.param_groups[0]['lr'] = lr\n\n            post_processed_batch_tuple = post_process_sequence_batch(batch)\n\n            input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n\n            output_sequences_batch_var =  Variable( output_sequences_batch.contiguous().view(-1).cuda() )\n\n            input_sequences_batch_var = Variable( input_sequences_batch.cuda() )\n\n            optimizer.zero_grad()\n\n            logits, _ = model(input_sequences_batch_var, sequences_lengths)\n\n            loss = criterion(logits, output_sequences_batch_var)\n            loss_list.append(loss.item())\n            epoch_loss.append(loss.item())\n            loss.backward()\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n\n            optimizer.step()\n\n        current_trn_epoch = sum(epoch_loss)/len(trainset_loader)\n        print('Training Loss: Epoch:',epoch_number,':', current_trn_epoch)\n\n        current_val_loss = validate(model)\n        print('Validation Loss: Epoch:',epoch_number,':', current_val_loss)\n        print('')\n\n        val_list.append(current_val_loss)\n\n        if current_val_loss < best_val_loss:\n\n            torch.save(model.state_dict(), 'music_model_padfront_regularized.pth')\n            best_val_loss = current_val_loss\n    return best_val_loss","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-26T16:19:05.465667Z","start_time":"2018-11-26T16:16:20.312589Z"},"trusted":true},"cell_type":"code","source":"rnn = RNN(input_size=88, hidden_size=512, num_classes=88)\nrnn = rnn.cuda()\nlrs_triangular = get_triangular_lr(1e-2, 1e-2*3.5, len(trainset_loader))\nbest_val_loss = train_model(rnn, lrs_triangular, epochs_number=10)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-26T16:21:52.147229Z","start_time":"2018-11-26T16:19:05.61726Z"},"trusted":true},"cell_type":"code","source":"lrs_triangular = get_triangular_lr(1e-3, 1e-2, len(trainset_loader))\nbest_val_loss = train_model(rnn, lrs_triangular, epochs_number=10, wd=1e-4, best_val_loss=best_val_loss)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-26T16:24:39.239756Z","start_time":"2018-11-26T16:21:52.289423Z"},"trusted":true},"cell_type":"code","source":"lrs_triangular = get_triangular_lr(1e-4, 1e-2, len(trainset_loader))\nbest_val_loss = train_model(rnn, lrs_triangular, epochs_number=10, wd=1e-4*5, best_val_loss=best_val_loss)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation"},{"metadata":{"ExecuteTime":{"end_time":"2018-11-26T16:25:11.202962Z","start_time":"2018-11-26T16:25:11.189993Z"},"trusted":true},"cell_type":"code","source":"def sample_from_piano_rnn(rnn, sample_length=4, temperature=0.8, starting_sequence=None):\n\n    if starting_sequence is None:\n                \n        current_sequence_input = torch.zeros(1, 1, 88)\n        current_sequence_input[0, 0, 40] = 1\n        current_sequence_input[0, 0, 50] = 0\n        current_sequence_input[0, 0, 56] = 0\n        current_sequence_input = Variable(current_sequence_input.cuda())\n    else:\n        current_sequence_input = starting_sequence\n        \n    final_output_sequence = [current_sequence_input.data.squeeze(1)]\n\n    hidden = None\n\n    for i in range(sample_length):\n\n        output, hidden = rnn(current_sequence_input, [1], hidden)\n\n        probabilities = nn.functional.softmax(output.div(temperature), dim=1)\n\n        current_sequence_input = torch.multinomial(probabilities.data, 1).squeeze().unsqueeze(0).unsqueeze(1)\n\n        current_sequence_input = Variable(current_sequence_input.float())\n\n        final_output_sequence.append(current_sequence_input.data.squeeze(1))\n\n    sampled_sequence = torch.cat(final_output_sequence, dim=0).cpu().numpy()\n    \n    return sampled_sequence","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-26T16:25:16.419484Z","start_time":"2018-11-26T16:25:14.709507Z"},"trusted":true},"cell_type":"code","source":"testset = NotesGenerationDataset('/kaggle/input/maestro/test/', longest_sequence_length=None)\n\ntestset_loader = torch.utils.data.DataLoader(testset, batch_size=1,shuffle=True, drop_last=False)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-26T16:25:18.156725Z","start_time":"2018-11-26T16:25:17.90675Z"},"trusted":true},"cell_type":"code","source":"batch = next(iter(testset_loader))\npost_processed_batch_tuple = post_process_sequence_batch(batch)\n\ninput_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n\noutput_sequences_batch_var =  output_sequences_batch.contiguous().view(-1).cuda()\n\ninput_sequences_batch_var = input_sequences_batch.cuda()\ninput_sequences_batch_var.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save model\ntorch.save(rnn.state_dict(), 'model_10e.pth')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FileLink('model_10e.pth')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-26T16:25:33.195424Z","start_time":"2018-11-26T16:25:32.923613Z"},"trusted":true},"cell_type":"code","source":"#plt.imshow(input_sequences_batch_var.reshape((input_sequences_batch_var.shape[0],88)).transpose(0,1))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-26T16:25:36.632928Z","start_time":"2018-11-26T16:25:34.966547Z"},"trusted":true},"cell_type":"code","source":"sample = sample_from_piano_rnn(rnn, sample_length=200, temperature=1,starting_sequence=None).transpose()\nio.imshow(sample)\nmidiwrite('sample_maestro_10e_1.mid', sample.transpose(), dt=0.3)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-26T16:30:38.850609Z","start_time":"2018-11-26T16:30:38.843627Z"},"trusted":true},"cell_type":"code","source":"FileLink('sample_maestro_10e_1.mid')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":4}