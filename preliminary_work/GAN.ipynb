{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import notebook, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train_128.pickle', 'rb') as train_pickle:\n",
    "    train_data = pickle.load(train_pickle)\n",
    "    \n",
    "with open('data/valid_128.pickle', 'rb') as valid_pickle:\n",
    "    valid_data = pickle.load(valid_pickle)\n",
    "\n",
    "with open('data/test_128.pickle', 'rb') as test_pickle:\n",
    "    test_data = pickle.load(test_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiResBlock(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super().__init__()\n",
    "        self.conv1x1 = nn.Conv2d(ch_in, ch_out, kernel_size=1, padding=0)\n",
    "        self.bnorm1x1 = nn.BatchNorm2d(ch_out,track_running_stats=False)\n",
    "        self.fconv = nn.Conv2d(ch_in, ch_out//6, kernel_size=3, padding=1)\n",
    "        self.fbnorm = nn.BatchNorm2d(ch_out//6,track_running_stats=False)\n",
    "        self.sconv = nn.Conv2d(ch_out//6, ch_out//3, kernel_size=3, padding=1)\n",
    "        self.sbnorm = nn.BatchNorm2d(ch_out//3,track_running_stats=False)\n",
    "        self.tconv = nn.Conv2d(ch_out//3, ch_out//2+1, kernel_size=3, padding=1)\n",
    "        self.tbnorm = nn.BatchNorm2d(ch_out//2+1,track_running_stats=False)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        res1x1 = self.relu(self.bnorm1x1(self.conv1x1(x)))\n",
    "        #print(\"res1x1 done\")\n",
    "        first = self.relu(self.fbnorm(self.fconv(x)))\n",
    "        #print(\"fconv done\")\n",
    "        second = self.relu(self.sbnorm(self.sconv(first)))\n",
    "        third = self.relu(self.tbnorm(self.tconv(second)))\n",
    "        resconv = torch.cat((first,second,third),dim=1)\n",
    "        y = res1x1+resconv\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'class Generator_Res(nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.tr1 = MultiResBlock(512, 256) \\n        self.tr2 = MultiResBlock(256, 128)\\n        self.tr3 = MultiResBlock(128, 64)\\n        self.tr4 = MultiResBlock(64, 32)\\n        self.tr5 = MultiResBlock(32, 16)\\n        self.tr6 = MultiResBlock(16, 8)\\n        self.tr7 = nn.Conv2d(8, 1, kernel_size = 3, stride = 1, padding = 1)\\n        self.relu = nn.ReLU()\\n        self.fc = nn.Linear(1024, 512*4*4)\\n        self.sigmoid = nn.Sigmoid()\\n        self.upsample = nn.Upsample(scale_factor = 2)\\n        \\n        self.up1 = nn.ConvTranspose2d(256, 256, kernel_size = 4, stride = 2, padding = 1)\\n        self.up2 = nn.ConvTranspose2d(128, 128, kernel_size = 4, stride = 2, padding = 1)\\n        self.up3 = nn.ConvTranspose2d(64, 64, kernel_size = 4, stride = 2, padding = 1)\\n        self.up4 = nn.ConvTranspose2d(32, 32, kernel_size = 4, stride = 2, padding = 1)\\n        self.up5 = nn.ConvTranspose2d(16, 16, kernel_size = 4, stride = 2, padding = 1)\\n        \\n\\n\\n    def forward(self, z):\\n        z = self.fc(z)\\n        z = z.view(-1, 512, 4, 4)\\n        z = self.tr1(z)\\n        z = self.up1(z) # 8x8\\n        z = self.tr2(z)\\n        z = self.up2(z) # 16x16\\n        z = self.tr3(z)\\n        z = self.up3(z) # 32x32\\n        z = self.tr4(z)\\n        z = self.up4(z) # 64x64\\n        z = self.tr5(z)\\n        z = self.up5(z) # 128x128\\n        z = self.tr6(z)\\n        z = self.tr7(z)\\n        z = self.sigmoid(z)\\n        return z'"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "'''class Generator_Res(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tr1 = MultiResBlock(512, 256) \n",
    "        self.tr2 = MultiResBlock(256, 128)\n",
    "        self.tr3 = MultiResBlock(128, 64)\n",
    "        self.tr4 = MultiResBlock(64, 32)\n",
    "        self.tr5 = MultiResBlock(32, 16)\n",
    "        self.tr6 = MultiResBlock(16, 8)\n",
    "        self.tr7 = nn.Conv2d(8, 1, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(1024, 512*4*4)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.upsample = nn.Upsample(scale_factor = 2)\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(256, 256, kernel_size = 4, stride = 2, padding = 1)\n",
    "        self.up2 = nn.ConvTranspose2d(128, 128, kernel_size = 4, stride = 2, padding = 1)\n",
    "        self.up3 = nn.ConvTranspose2d(64, 64, kernel_size = 4, stride = 2, padding = 1)\n",
    "        self.up4 = nn.ConvTranspose2d(32, 32, kernel_size = 4, stride = 2, padding = 1)\n",
    "        self.up5 = nn.ConvTranspose2d(16, 16, kernel_size = 4, stride = 2, padding = 1)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.fc(z)\n",
    "        z = z.view(-1, 512, 4, 4)\n",
    "        z = self.tr1(z)\n",
    "        z = self.up1(z) # 8x8\n",
    "        z = self.tr2(z)\n",
    "        z = self.up2(z) # 16x16\n",
    "        z = self.tr3(z)\n",
    "        z = self.up3(z) # 32x32\n",
    "        z = self.tr4(z)\n",
    "        z = self.up4(z) # 64x64\n",
    "        z = self.tr5(z)\n",
    "        z = self.up5(z) # 128x128\n",
    "        z = self.tr6(z)\n",
    "        z = self.tr7(z)\n",
    "        z = self.sigmoid(z)\n",
    "        return z'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator_Res2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tr1 = MultiResBlock(256, 128)\n",
    "        self.tr2 = MultiResBlock(128, 64)\n",
    "        self.tr3 = MultiResBlock(64, 32)\n",
    "        self.tr4 = MultiResBlock(32, 16)\n",
    "        self.tr5 = MultiResBlock(16, 8)\n",
    "        self.tr6 = nn.Conv2d(8, 1, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(1024, 256*16*16)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.upsample = nn.Upsample(scale_factor = 2)\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.fc(z)\n",
    "        z = z.view(-1, 256, 16, 16)\n",
    "        z = self.tr1(z)\n",
    "        z = self.upsample(z)\n",
    "        z = self.tr2(z)\n",
    "        z = self.upsample(z)\n",
    "        z = self.tr3(z)\n",
    "        z = self.upsample(z)\n",
    "        z = self.tr4(z)\n",
    "        z = self.tr5(z)\n",
    "        z = self.tr6(z)\n",
    "        z = self.sigmoid(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        Conv2d: w' = (w - k +2p)/s + 1\n",
    "        ConvTr2d: w = s(w'-1) + k -2p\n",
    "        '''\n",
    "        #[b, 512, 4, 4]\n",
    "        self.tr1 = nn.ConvTranspose2d(512, 384, kernel_size = 4, stride = 2, padding = 1) # [b, 384, 8, 8]\n",
    "        self.tr2 = nn.ConvTranspose2d(384, 256, kernel_size = 3, stride = 1, padding = 1) # [b, 256, 8, 8]\n",
    "        self.tr3 = nn.ConvTranspose2d(256, 192, kernel_size = 4, padding = 1, stride = 2) # [b, 192, 16, 16]\n",
    "        self.tr4 = nn.ConvTranspose2d(192, 128, kernel_size = 3, stride = 1, padding = 1) # [b, 128, 16, 16]\n",
    "        self.tr5 = nn.ConvTranspose2d(128, 96, kernel_size = 4, padding = 1, stride = 2) # [b, 96, 32, 32]\n",
    "        self.tr6 = nn.ConvTranspose2d(96, 64, kernel_size = 3, stride = 1, padding = 1) # [b, 64, 32, 32]\n",
    "        self.tr7 = nn.ConvTranspose2d(64, 48, kernel_size = 4, padding = 1, stride = 2) # [b, 48, 32, 32]\n",
    "        self.tr8 = nn.ConvTranspose2d(48, 32, kernel_size = 3, stride = 1, padding = 1) # [b, 32, 64, 64]\n",
    "        self.tr9 = nn.ConvTranspose2d(32, 16, kernel_size = 4, padding = 1, stride = 2) # [b, 16, 64, 64]\n",
    "        self.tr10 = nn.ConvTranspose2d(16, 8, kernel_size = 3, stride = 1, padding = 1) # [b, 8, 128, 128]\n",
    "        self.tr11 = nn.ConvTranspose2d(8, 4, kernel_size = 3, padding = 1, stride = 1) # [b, 4, 128, 128]\n",
    "        self.tr12 = nn.ConvTranspose2d(4, 1, kernel_size = 3, stride = 1, padding = 1) # [b, 1, 128, 128]\n",
    "        \n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(384)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.bn3 = nn.BatchNorm2d(192)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.bn5 = nn.BatchNorm2d(96)\n",
    "        self.bn6 = nn.BatchNorm2d(64)\n",
    "        self.bn7 = nn.BatchNorm2d(48)\n",
    "        self.bn8 = nn.BatchNorm2d(32)\n",
    "        self.bn9 = nn.BatchNorm2d(16)\n",
    "        self.bn10 = nn.BatchNorm2d(8)\n",
    "        self.bn11 = nn.BatchNorm2d(4)\n",
    "        \n",
    "        '''\n",
    "        self.tr1 = nn.ConvTranspose2d(256, 128, kernel_size = 4, stride = 2, padding = 1)\n",
    "        self.tr2 = nn.ConvTranspose2d(128, 64, kernel_size = 4, stride = 2, padding = 1)\n",
    "        self.tr3 = nn.ConvTranspose2d(64, 32, kernel_size = 4, stride = 2, padding = 1)\n",
    "        self.tr4 = nn.ConvTranspose2d(32, 16, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.tr5 = nn.ConvTranspose2d(16, 8, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.tr6 = nn.ConvTranspose2d(8, 1, kernel_size = 3, stride = 1, padding = 1)'''\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(1024, 512*4*4)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.fc(z)\n",
    "        z = z.view(-1, 512, 4, 4)\n",
    "        z1 = self.bn1(self.relu(self.tr1(z)))\n",
    "        # z = [b, 512, 4, 4], z1 = [b, 384, 8, 8], z2 [b, 256, 8, 8] -> [b, 640, 8, 8]\n",
    "        z2 = self.bn2(self.relu(self.tr2(z1)))\n",
    "        z3 = self.bn3(self.relu(self.tr3(z2)))\n",
    "        z4 = self.bn4(self.relu(self.tr4(z3)))\n",
    "        z5 = self.bn5(self.relu(self.tr5(z4)))\n",
    "        z6 = self.bn6(self.relu(self.tr6(z5)))\n",
    "        z7 = self.bn7(self.relu(self.tr7(z6)))\n",
    "        z8 = self.bn8(self.relu(self.tr8(z7)))\n",
    "        z9 = self.bn9(self.relu(self.tr9(z8)))\n",
    "        z10 = self.bn10(self.relu(self.tr10(z9)))\n",
    "        z11 = self.bn11(self.relu(self.tr11(z10)))\n",
    "        z12 = self.sigmoid(self.tr12(z11))\n",
    "        return z12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, kernel_size = 7, stride =2)#, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(4, 8, kernel_size = 5, stride = 2)#, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(8, 16, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.fc1 = nn.Linear(16*29*29, 1)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.maxpool(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.maxpool(x)\n",
    "        x = x.view(-1, 29*29*16)\n",
    "        logit = self.fc1(x)\n",
    "        #logit = self.fc2(x)\n",
    "        x = self.sigmoid(logit)\n",
    "        return x, logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator_2(nn.Module):\n",
    "    '''\n",
    "    Conv2d: w' = (w - k +2p)/s + 1\n",
    "    ConvTr2d: w = s(w'-1) + k -2p\n",
    "    '''    \n",
    "    def __init__(self, in_size = 1, ndf = 64):\n",
    "        super().__init__()       \n",
    "        self.in_size = in_size\n",
    "        self.ndf = ndf\n",
    "        self.main = nn.Sequential(\n",
    "            # input size is in_size x 128 x 128\n",
    "            nn.Conv2d(in_size, self.ndf, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: ndf x 64 x 64\n",
    "            nn.Conv2d(self.ndf, self.ndf, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: ndf x 32 x 32\n",
    "            nn.Conv2d(self.ndf, self.ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf * 2) x 16 x 16\n",
    "            nn.Conv2d(self.ndf * 2, self.ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf * 4) x 8 x 8\n",
    "            nn.Conv2d(self.ndf * 4, self.ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf * 8) x 4 x 4\n",
    "            nn.Conv2d(self.ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "            # state size: 1 x 1 x 1\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.main(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mean(x):\n",
    "    output = torch.mean(x,0, keepdim = False)\n",
    "    output = torch.mean(output,-1, keepdim = False)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(discriminator, generator, optimizerD, optimizerG, d_criterion, g_criterion, data, batch_size, epoch, device):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    lossD = []\n",
    "    lossG = []\n",
    "    sum_D_x = 0\n",
    "    sum_loss_D = 0\n",
    "    sum_loss_G = 0\n",
    "    it = 0\n",
    "    for X in batch_generator(data, batch_size, shuffle=True):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        #train with real\n",
    "        X = torch.from_numpy(X).float().to(device)\n",
    "        X = X/127\n",
    "        X = torch.unsqueeze(X,1)\n",
    "        discriminator.zero_grad()\n",
    "        \n",
    "        D, D_logits = discriminator(X)\n",
    "        \n",
    "        \n",
    "\n",
    "        #####loss\n",
    "        d_loss_real = reduce_mean(d_criterion(D_logits, 0.9*torch.ones_like(D)))\n",
    "        d_loss_real.backward()#retain_graph=True)\n",
    "        D_x = D.mean().item()\n",
    "        sum_D_x += D_x \n",
    "        \n",
    "        \n",
    "        #train with fake\n",
    "        \n",
    "        noise = torch.randn(batch_size, 1024, device=device)\n",
    "        \n",
    "\n",
    "        X = generator(noise)\n",
    "        D_, D_logits_ = discriminator(X)\n",
    "        d_loss_fake = reduce_mean(d_criterion(D_logits_, torch.zeros_like(D_)))\n",
    "        d_loss_fake.backward(retain_graph=True)\n",
    "        \n",
    "        D_G_z1 = D_.mean().item()\n",
    "        \n",
    "        errD = d_loss_fake + d_loss_real\n",
    "        errD = errD.item()\n",
    "        \n",
    "        sum_loss_D += errD\n",
    "        \n",
    "        optimizerD.step()\n",
    "        \n",
    "        \n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        generator.zero_grad()\n",
    "                \n",
    "        D_, D_logits_ = discriminator(X)\n",
    "\n",
    "        ###loss\n",
    "        errG = reduce_mean(g_criterion(D_logits_, torch.ones_like(D_)))\n",
    "        \n",
    "        errG.backward()#retain_graph=True)\n",
    "        D_G_z2 = D_.mean().item()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        sum_loss_G += errG\n",
    "        it += 1\n",
    "        if it % 100 == 0:\n",
    "            print(f'Train | D loss={errD:.6f} | G loss:{errG:.6f} | D(x):{D_x:.6f} | D(G(z)) = {D_G_z1:.4f} / {D_G_z2:.4f}')\n",
    "    print(f'Train | epoch={epoch:03d} | D loss={sum_loss_D:.6f} | G loss:{sum_loss_G:.6f}')\n",
    "    \n",
    "\n",
    "\n",
    "def validate(discriminator, generator, d_criterion, g_criterion, data, batch_size, epoch, device):\n",
    "    discriminator.eval()\n",
    "    generator.eval()\n",
    "    lossD = []\n",
    "    lossG = []\n",
    "    sum_D_x = 0\n",
    "    sum_loss_D = 0\n",
    "    sum_loss_G = 0\n",
    "    it = 0\n",
    "    with torch.no_grad():\n",
    "        for X in batch_generator(data, batch_size, shuffle=True):\n",
    "            ############################\n",
    "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            ###########################\n",
    "            #train with real\n",
    "            X = torch.from_numpy(X).float().to(device)\n",
    "            X = X/127\n",
    "            X = torch.unsqueeze(X,1)\n",
    "\n",
    "            D, D_logits = discriminator(X)\n",
    "\n",
    "\n",
    "\n",
    "            #####loss\n",
    "            d_loss_real = reduce_mean(d_criterion(D_logits, 0.9*torch.ones_like(D)))\n",
    "            D_x = D.mean().item()\n",
    "            sum_D_x += D_x \n",
    "\n",
    "\n",
    "            #train with fake\n",
    "\n",
    "            noise = torch.randn(batch_size, 1024, device=device)\n",
    "\n",
    "\n",
    "            X = generator(noise)\n",
    "            D_, D_logits_ = discriminator(X)\n",
    "            d_loss_fake = reduce_mean(d_criterion(D_logits_, torch.zeros_like(D_)))\n",
    "\n",
    "            D_G_z1 = D_.mean().item()\n",
    "\n",
    "            errD = d_loss_fake + d_loss_real\n",
    "            errD = errD.item()\n",
    "\n",
    "            sum_loss_D += errD\n",
    "\n",
    "            ############################\n",
    "            # (2) Update G network: maximize log(D(G(z)))\n",
    "            ###########################\n",
    "            D_, D_logits_ = discriminator(X)\n",
    "\n",
    "            ###loss\n",
    "            errG = reduce_mean(g_criterion(D_logits_, torch.ones_like(D_)))\n",
    "            D_G_z2 = D_.mean().item()\n",
    "            sum_loss_G += errG\n",
    "            it += 1\n",
    "            if it % 100 == 0:\n",
    "                print(f'Validation | D loss={errD:.6f} | G loss:{errG:.6f} | D(x):{D_x:.6f} | D(G(z)) = {D_G_z1:.4f} / {D_G_z2:.4f}')\n",
    "        print(f'Validation | epoch={epoch:03d} | D loss={sum_loss_D:.6f} | G loss:{sum_loss_G:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2(discriminator, generator, optimizerD, optimizerG, d_criterion, g_criterion, data, batch_size, epoch, device):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    lossD = []\n",
    "    lossG = []\n",
    "    sum_D_x = 0\n",
    "    sum_loss_D = 0\n",
    "    sum_loss_G = 0\n",
    "    it = 0\n",
    "    for X in batch_generator(data, batch_size, shuffle=True):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        #train with real\n",
    "        X = torch.from_numpy(X).float().to(device)\n",
    "        X = X/127\n",
    "        X = torch.unsqueeze(X,1)\n",
    "        discriminator.zero_grad()\n",
    "        \n",
    "        D = discriminator(X).view(-1)\n",
    "        \n",
    "        \n",
    "\n",
    "        #####loss\n",
    "        d_loss_real = reduce_mean(d_criterion(D, 0.9*torch.ones_like(D)))\n",
    "        d_loss_real.backward()#retain_graph=True)\n",
    "        D_x = D.mean().item()\n",
    "        sum_D_x += D_x \n",
    "        \n",
    "        \n",
    "        #train with fake\n",
    "        \n",
    "        noise = torch.randn(batch_size, 1024, device=device)\n",
    "        \n",
    "\n",
    "        X = generator(noise)\n",
    "        D_ = discriminator(X).view(-1)\n",
    "        d_loss_fake = reduce_mean(d_criterion(D_, torch.zeros_like(D_)))\n",
    "        d_loss_fake.backward(retain_graph=True)\n",
    "        \n",
    "        D_G_z1 = D_.mean().item()\n",
    "        \n",
    "        errD = d_loss_fake + d_loss_real\n",
    "        errD = errD.item()\n",
    "        \n",
    "        sum_loss_D += errD\n",
    "        \n",
    "        optimizerD.step()\n",
    "        \n",
    "        \n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        generator.zero_grad()\n",
    "                \n",
    "        D_ = discriminator(X).view(-1)\n",
    "\n",
    "        ###loss\n",
    "        errG = reduce_mean(g_criterion(D_, torch.ones_like(D_)))\n",
    "        \n",
    "        errG.backward()#retain_graph=True)\n",
    "        D_G_z2 = D_.mean().item()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        sum_loss_G += errG\n",
    "        it += 1\n",
    "        if it % 100 == 0:\n",
    "            print(f'Train | D loss={errD:.6f} | G loss:{errG:.6f} | D(x):{D_x:.6f} | D(G(z)) = {D_G_z1:.4f} / {D_G_z2:.4f}')\n",
    "    print(f'Train | epoch={epoch:03d} | D loss={sum_loss_D:.6f} | G loss:{sum_loss_G:.6f}')\n",
    "    \n",
    "\n",
    "\n",
    "def validate2(discriminator, generator, d_criterion, g_criterion, data, batch_size, epoch, device):\n",
    "    discriminator.eval()\n",
    "    generator.eval()\n",
    "    lossD = []\n",
    "    lossG = []\n",
    "    sum_D_x = 0\n",
    "    sum_loss_D = 0\n",
    "    sum_loss_G = 0\n",
    "    it = 0\n",
    "    with torch.no_grad():\n",
    "        for X in batch_generator(data, batch_size, shuffle=True):\n",
    "            ############################\n",
    "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            ###########################\n",
    "            #train with real\n",
    "            X = torch.from_numpy(X).float().to(device)\n",
    "            X = X/127\n",
    "            X = torch.unsqueeze(X,1)\n",
    "\n",
    "            D = discriminator(X).view(-1)\n",
    "\n",
    "\n",
    "\n",
    "            #####loss\n",
    "            d_loss_real = reduce_mean(d_criterion(D, 0.9*torch.ones_like(D)))\n",
    "            D_x = D.mean().item()\n",
    "            sum_D_x += D_x \n",
    "\n",
    "\n",
    "            #train with fake\n",
    "\n",
    "            noise = torch.randn(batch_size, 1024, device=device)\n",
    "\n",
    "\n",
    "            X = generator(noise)\n",
    "            D_ = discriminator(X).view(-1)\n",
    "            d_loss_fake = reduce_mean(d_criterion(D_, torch.zeros_like(D_)))\n",
    "\n",
    "            D_G_z1 = D_.mean().item()\n",
    "\n",
    "            errD = d_loss_fake + d_loss_real\n",
    "            errD = errD.item()\n",
    "\n",
    "            sum_loss_D += errD\n",
    "\n",
    "            ############################\n",
    "            # (2) Update G network: maximize log(D(G(z)))\n",
    "            ###########################\n",
    "            D_ = discriminator(X).view(-1)\n",
    "\n",
    "            ###loss\n",
    "            errG = reduce_mean(g_criterion(D_, torch.ones_like(D_)))\n",
    "            D_G_z2 = D_.mean().item()\n",
    "            sum_loss_G += errG\n",
    "            it += 1\n",
    "            if it % 100 == 0:\n",
    "                print(f'Validation | D loss={errD:.6f} | G loss:{errG:.6f} | D(x):{D_x:.6f} | D(G(z)) = {D_G_z1:.4f} / {D_G_z2:.4f}')\n",
    "        print(f'Validation | epoch={epoch:03d} | D loss={sum_loss_D:.6f} | G loss:{sum_loss_G:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(data, batch_size=1, shuffle=False):\n",
    "    nsamples = len(data)\n",
    "    if shuffle:\n",
    "        perm = np.random.permutation(nsamples)\n",
    "    else:\n",
    "        perm = range(nsamples)\n",
    "\n",
    "    for i in range(0, nsamples, batch_size):\n",
    "        batch_idx = perm[i:i+batch_size]\n",
    "        yield data[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "s:3.466489 | D(x):0.836530 | D(G(z)) = 0.0312 / 0.0312\nValidation | D loss=0.538645 | G loss:3.468076 | D(x):0.829236 | D(G(z)) = 0.0312 / 0.0312\nValidation | D loss=0.447932 | G loss:3.470951 | D(x):0.860061 | D(G(z)) = 0.0311 / 0.0311\nValidation | D loss=0.492305 | G loss:3.465893 | D(x):0.840629 | D(G(z)) = 0.0312 / 0.0312\nValidation | D loss=0.419216 | G loss:3.469617 | D(x):0.875769 | D(G(z)) = 0.0311 / 0.0311\nValidation | D loss=0.464862 | G loss:3.465104 | D(x):0.860023 | D(G(z)) = 0.0313 / 0.0313\nValidation | D loss=0.449150 | G loss:3.465120 | D(x):0.856109 | D(G(z)) = 0.0313 / 0.0313\nValidation | D loss=0.410586 | G loss:3.467199 | D(x):0.872097 | D(G(z)) = 0.0312 / 0.0312\nValidation | D loss=0.416868 | G loss:3.463191 | D(x):0.868181 | D(G(z)) = 0.0313 / 0.0313\nValidation | D loss=0.456147 | G loss:3.469190 | D(x):0.853632 | D(G(z)) = 0.0311 / 0.0311\nValidation | epoch=003 | D loss=510.839032 | G loss:3913.112793\nTrain | D loss=0.438477 | G loss:3.169806 | D(x):0.875213 | D(G(z)) = 0.0468 / 0.0420\nTrain | D loss=0.451569 | G loss:3.466832 | D(x):0.834501 | D(G(z)) = 0.0263 / 0.0312\nTrain | D loss=0.450504 | G loss:3.247528 | D(x):0.895006 | D(G(z)) = 0.0407 / 0.0389\nTrain | D loss=0.408984 | G loss:3.647221 | D(x):0.846415 | D(G(z)) = 0.0248 / 0.0261\nTrain | D loss=0.424931 | G loss:3.393776 | D(x):0.866421 | D(G(z)) = 0.0336 / 0.0336\nTrain | D loss=0.400836 | G loss:3.847182 | D(x):0.878008 | D(G(z)) = 0.0239 / 0.0213\nTrain | D loss=0.422089 | G loss:3.831869 | D(x):0.878833 | D(G(z)) = 0.0249 / 0.0217\nTrain | D loss=0.407242 | G loss:3.669290 | D(x):0.878766 | D(G(z)) = 0.0261 / 0.0255\nTrain | D loss=0.407690 | G loss:3.856136 | D(x):0.866347 | D(G(z)) = 0.0210 / 0.0211\nTrain | D loss=1.349832 | G loss:0.781214 | D(x):0.484523 | D(G(z)) = 0.4653 / 0.4579\nTrain | D loss=1.022137 | G loss:1.027462 | D(x):0.626636 | D(G(z)) = 0.3711 / 0.3580\nTrain | epoch=004 | D loss=658.144108 | G loss:3531.894775\nValidation | D loss=2.293804 | G loss:0.487371 | D(x):0.250023 | D(G(z)) = 0.6153 / 0.6153\nValidation | D loss=2.324149 | G loss:0.496987 | D(x):0.252108 | D(G(z)) = 0.6094 / 0.6094\nValidation | D loss=2.313417 | G loss:0.490006 | D(x):0.252416 | D(G(z)) = 0.6135 / 0.6135\nValidation | D loss=2.337445 | G loss:0.485353 | D(x):0.249832 | D(G(z)) = 0.6160 / 0.6160\nValidation | D loss=2.261582 | G loss:0.489960 | D(x):0.270231 | D(G(z)) = 0.6135 / 0.6135\nValidation | D loss=2.317201 | G loss:0.483041 | D(x):0.254717 | D(G(z)) = 0.6174 / 0.6174\nValidation | D loss=2.344332 | G loss:0.485847 | D(x):0.245070 | D(G(z)) = 0.6158 / 0.6158\nValidation | D loss=2.297612 | G loss:0.482688 | D(x):0.262244 | D(G(z)) = 0.6178 / 0.6178\nValidation | D loss=2.311315 | G loss:0.484100 | D(x):0.256261 | D(G(z)) = 0.6169 / 0.6169\nValidation | D loss=2.229948 | G loss:0.491639 | D(x):0.294108 | D(G(z)) = 0.6124 / 0.6124\nValidation | D loss=2.306783 | G loss:0.481508 | D(x):0.261390 | D(G(z)) = 0.6182 / 0.6182\nValidation | epoch=004 | D loss=2601.971442 | G loss:551.750366\nTrain | D loss=1.351882 | G loss:0.801776 | D(x):0.465570 | D(G(z)) = 0.4504 / 0.4486\nTrain | D loss=1.372793 | G loss:0.852636 | D(x):0.433996 | D(G(z)) = 0.4279 / 0.4264\nTrain | D loss=1.207841 | G loss:1.000016 | D(x):0.483319 | D(G(z)) = 0.3800 / 0.3708\nTrain | D loss=1.272881 | G loss:0.846943 | D(x):0.497481 | D(G(z)) = 0.4321 / 0.4288\nTrain | D loss=1.360177 | G loss:0.743946 | D(x):0.489873 | D(G(z)) = 0.4772 / 0.4753\nTrain | D loss=1.594370 | G loss:0.842990 | D(x):0.433675 | D(G(z)) = 0.5407 / 0.4311\nTrain | D loss=0.826090 | G loss:2.414217 | D(x):0.519622 | D(G(z)) = 0.0925 / 0.0894\nTrain | D loss=1.204133 | G loss:1.104985 | D(x):0.492958 | D(G(z)) = 0.3538 / 0.3316\nTrain | D loss=0.446976 | G loss:3.266294 | D(x):0.893720 | D(G(z)) = 0.0414 / 0.0381\nTrain | D loss=0.408926 | G loss:3.632731 | D(x):0.851093 | D(G(z)) = 0.0257 / 0.0264\nTrain | D loss=0.484961 | G loss:3.203081 | D(x):0.827672 | D(G(z)) = 0.0269 / 0.0406\nTrain | epoch=005 | D loss=1215.428759 | G loss:1945.081909\nValidation | D loss=0.431935 | G loss:3.689305 | D(x):0.871001 | D(G(z)) = 0.0250 / 0.0250\nValidation | D loss=0.422374 | G loss:3.689778 | D(x):0.883293 | D(G(z)) = 0.0250 / 0.0250\nValidation | D loss=0.450065 | G loss:3.689787 | D(x):0.831428 | D(G(z)) = 0.0250 / 0.0250\nValidation | D loss=0.389683 | G loss:3.689033 | D(x):0.894337 | D(G(z)) = 0.0250 / 0.0250\nValidation | D loss=0.400256 | G loss:3.689075 | D(x):0.858129 | D(G(z)) = 0.0250 / 0.0250\nValidation | D loss=0.436936 | G loss:3.689660 | D(x):0.857705 | D(G(z)) = 0.0250 / 0.0250\nValidation | D loss=0.446885 | G loss:3.689292 | D(x):0.834697 | D(G(z)) = 0.0250 / 0.0250\nValidation | D loss=0.443908 | G loss:3.689706 | D(x):0.845378 | D(G(z)) = 0.0250 / 0.0250\nValidation | D loss=0.464926 | G loss:3.689720 | D(x):0.823469 | D(G(z)) = 0.0250 / 0.0250\nValidation | D loss=0.470378 | G loss:3.689569 | D(x):0.854397 | D(G(z)) = 0.0250 / 0.0250\nValidation | D loss=0.381367 | G loss:3.689926 | D(x):0.886142 | D(G(z)) = 0.0250 / 0.0250\nValidation | epoch=005 | D loss=493.449130 | G loss:4165.390137\nTrain | D loss=1.973229 | G loss:0.788932 | D(x):0.216891 | D(G(z)) = 0.3837 / 0.4555\nTrain | D loss=1.352018 | G loss:0.932873 | D(x):0.446888 | D(G(z)) = 0.4217 / 0.3935\nTrain | D loss=1.029805 | G loss:1.144127 | D(x):0.571154 | D(G(z)) = 0.3273 / 0.3185\nTrain | D loss=0.993950 | G loss:1.096207 | D(x):0.592178 | D(G(z)) = 0.3370 / 0.3342\nTrain | D loss=1.217683 | G loss:0.873369 | D(x):0.523131 | D(G(z)) = 0.4239 / 0.4176\nTrain | D loss=0.533026 | G loss:2.704146 | D(x):0.796100 | D(G(z)) = 0.1085 / 0.0669\nTrain | D loss=1.271268 | G loss:0.812234 | D(x):0.511813 | D(G(z)) = 0.4427 / 0.4439\nTrain | D loss=1.409925 | G loss:0.861660 | D(x):0.410603 | D(G(z)) = 0.4260 / 0.4225\nTrain | D loss=1.273907 | G loss:0.872676 | D(x):0.481135 | D(G(z)) = 0.4197 / 0.4178\nTrain | D loss=1.373333 | G loss:0.791190 | D(x):0.455289 | D(G(z)) = 0.4536 / 0.4533\nTrain | D loss=1.385585 | G loss:0.772752 | D(x):0.458543 | D(G(z)) = 0.4633 / 0.4617\nTrain | epoch=006 | D loss=1457.070623 | G loss:1276.773926\nValidation | D loss=1.384233 | G loss:0.795051 | D(x):0.447249 | D(G(z)) = 0.4516 / 0.4516\nValidation | D loss=1.385607 | G loss:0.794842 | D(x):0.446581 | D(G(z)) = 0.4517 / 0.4517\nValidation | D loss=1.384770 | G loss:0.795512 | D(x):0.446728 | D(G(z)) = 0.4514 / 0.4514\nValidation | D loss=1.384515 | G loss:0.795268 | D(x):0.447004 | D(G(z)) = 0.4515 / 0.4515\nValidation | D loss=1.384766 | G loss:0.795043 | D(x):0.446943 | D(G(z)) = 0.4516 / 0.4516\nValidation | D loss=1.384958 | G loss:0.794928 | D(x):0.446898 | D(G(z)) = 0.4516 / 0.4516\nValidation | D loss=1.383253 | G loss:0.795678 | D(x):0.447489 | D(G(z)) = 0.4513 / 0.4513\nValidation | D loss=1.384469 | G loss:0.795180 | D(x):0.447051 | D(G(z)) = 0.4515 / 0.4515\nValidation | D loss=1.381513 | G loss:0.795181 | D(x):0.448661 | D(G(z)) = 0.4515 / 0.4515\nValidation | D loss=1.385365 | G loss:0.795445 | D(x):0.446444 | D(G(z)) = 0.4514 / 0.4514\nValidation | D loss=1.386467 | G loss:0.795002 | D(x):0.446040 | D(G(z)) = 0.4516 / 0.4516\nValidation | epoch=006 | D loss=1563.038216 | G loss:897.841736\nTrain | D loss=1.362304 | G loss:0.847298 | D(x):0.437079 | D(G(z)) = 0.4287 / 0.4286\nTrain | D loss=1.333372 | G loss:0.903025 | D(x):0.431439 | D(G(z)) = 0.4054 / 0.4053\nTrain | D loss=1.294255 | G loss:0.929363 | D(x):0.444051 | D(G(z)) = 0.3960 / 0.3948\nTrain | D loss=1.407857 | G loss:0.648845 | D(x):0.517112 | D(G(z)) = 0.5234 / 0.5226\nTrain | D loss=1.384022 | G loss:0.682742 | D(x):0.508863 | D(G(z)) = 0.5057 / 0.5052\nTrain | D loss=1.362664 | G loss:0.709483 | D(x):0.507761 | D(G(z)) = 0.4937 / 0.4919\nTrain | D loss=2.167701 | G loss:0.359553 | D(x):0.529085 | D(G(z)) = 0.7392 / 0.7135\nTrain | D loss=1.379139 | G loss:0.765771 | D(x):0.463949 | D(G(z)) = 0.4650 / 0.4650\nTrain | D loss=1.381488 | G loss:0.762288 | D(x):0.464277 | D(G(z)) = 0.4666 / 0.4666\nTrain | D loss=1.375100 | G loss:0.765091 | D(x):0.466602 | D(G(z)) = 0.4653 / 0.4653\nTrain | D loss=1.393977 | G loss:0.778177 | D(x):0.450358 | D(G(z)) = 0.4595 / 0.4592\nTrain | epoch=007 | D loss=1548.022407 | G loss:883.620361\nValidation | D loss=1.322068 | G loss:0.794107 | D(x):0.483786 | D(G(z)) = 0.4520 / 0.4520\nValidation | D loss=1.321584 | G loss:0.795448 | D(x):0.483444 | D(G(z)) = 0.4514 / 0.4514\nValidation | D loss=1.311860 | G loss:0.795426 | D(x):0.489345 | D(G(z)) = 0.4514 / 0.4514\nValidation | D loss=1.320142 | G loss:0.795611 | D(x):0.484131 | D(G(z)) = 0.4513 / 0.4513\nValidation | D loss=1.322615 | G loss:0.795326 | D(x):0.482939 | D(G(z)) = 0.4514 / 0.4514\nValidation | D loss=1.319015 | G loss:0.794812 | D(x):0.485280 | D(G(z)) = 0.4517 / 0.4517\nValidation | D loss=1.325379 | G loss:0.796272 | D(x):0.480706 | D(G(z)) = 0.4510 / 0.4510\nValidation | D loss=1.327334 | G loss:0.795160 | D(x):0.480119 | D(G(z)) = 0.4515 / 0.4515\nValidation | D loss=1.323074 | G loss:0.795765 | D(x):0.482244 | D(G(z)) = 0.4512 / 0.4512\nValidation | D loss=1.328251 | G loss:0.795119 | D(x):0.479409 | D(G(z)) = 0.4515 / 0.4515\nValidation | D loss=1.321620 | G loss:0.795034 | D(x):0.483653 | D(G(z)) = 0.4516 / 0.4516\nValidation | epoch=007 | D loss=1492.056876 | G loss:897.897522\nTrain | D loss=1.391670 | G loss:0.775295 | D(x):0.453654 | D(G(z)) = 0.4619 / 0.4606\nTrain | D loss=1.339118 | G loss:0.803375 | D(x):0.469779 | D(G(z)) = 0.4480 / 0.4478\nTrain | D loss=1.633369 | G loss:0.724211 | D(x):0.368831 | D(G(z)) = 0.4926 / 0.4848\nTrain | D loss=1.305454 | G loss:0.927159 | D(x):0.476383 | D(G(z)) = 0.4251 / 0.3961\nTrain | D loss=1.369935 | G loss:0.997199 | D(x):0.399262 | D(G(z)) = 0.3877 / 0.3689\nTrain | D loss=1.407917 | G loss:0.779137 | D(x):0.444914 | D(G(z)) = 0.4618 / 0.4588\nTrain | D loss=1.327913 | G loss:0.780475 | D(x):0.491185 | D(G(z)) = 0.4610 / 0.4583\nTrain | D loss=1.376810 | G loss:0.871308 | D(x):0.444261 | D(G(z)) = 0.4419 / 0.4184\nTrain | D loss=0.878379 | G loss:2.194709 | D(x):0.738905 | D(G(z)) = 0.3293 / 0.1121\nTrain | D loss=1.418010 | G loss:0.793016 | D(x):0.432436 | D(G(z)) = 0.4547 / 0.4526\nTrain | D loss=1.388191 | G loss:0.800843 | D(x):0.442672 | D(G(z)) = 0.4490 / 0.4490\nTrain | epoch=008 | D loss=1555.810838 | G loss:951.679993\nValidation | D loss=1.386715 | G loss:0.802880 | D(x):0.442534 | D(G(z)) = 0.4480 / 0.4480\nValidation | D loss=1.386264 | G loss:0.802549 | D(x):0.442869 | D(G(z)) = 0.4482 / 0.4482\nValidation | D loss=1.382732 | G loss:0.802689 | D(x):0.444618 | D(G(z)) = 0.4481 / 0.4481\nValidation | D loss=1.382426 | G loss:0.803021 | D(x):0.444650 | D(G(z)) = 0.4480 / 0.4480\nValidation | D loss=1.383266 | G loss:0.802597 | D(x):0.444377 | D(G(z)) = 0.4482 / 0.4482\nValidation | D loss=1.385386 | G loss:0.802823 | D(x):0.443184 | D(G(z)) = 0.4481 / 0.4481\nValidation | D loss=1.385341 | G loss:0.802468 | D(x):0.443322 | D(G(z)) = 0.4482 / 0.4482\nValidation | D loss=1.385522 | G loss:0.802783 | D(x):0.443102 | D(G(z)) = 0.4481 / 0.4481\nValidation | D loss=1.384846 | G loss:0.803058 | D(x):0.443337 | D(G(z)) = 0.4480 / 0.4480\nValidation | D loss=1.382356 | G loss:0.802792 | D(x):0.444777 | D(G(z)) = 0.4481 / 0.4481\nValidation | D loss=1.383175 | G loss:0.802893 | D(x):0.444299 | D(G(z)) = 0.4480 / 0.4480\nValidation | epoch=008 | D loss=1562.884910 | G loss:906.358276\nTrain | D loss=1.380181 | G loss:0.803715 | D(x):0.445578 | D(G(z)) = 0.4477 / 0.4477\nTrain | D loss=1.377781 | G loss:0.802962 | D(x):0.447178 | D(G(z)) = 0.4480 / 0.4480\nTrain | D loss=1.376882 | G loss:0.801499 | D(x):0.448328 | D(G(z)) = 0.4487 / 0.4487\nTrain | D loss=1.376531 | G loss:0.801400 | D(x):0.448559 | D(G(z)) = 0.4487 / 0.4487\nTrain | D loss=1.376440 | G loss:0.800385 | D(x):0.449076 | D(G(z)) = 0.4492 / 0.4492\nTrain | D loss=1.376626 | G loss:0.800213 | D(x):0.449055 | D(G(z)) = 0.4492 / 0.4492\nTrain | D loss=1.376508 | G loss:0.800378 | D(x):0.449020 | D(G(z)) = 0.4491 / 0.4492\nTrain | D loss=1.376413 | G loss:0.800661 | D(x):0.448949 | D(G(z)) = 0.4490 / 0.4490\nTrain | D loss=1.376219 | G loss:0.800332 | D(x):0.449198 | D(G(z)) = 0.4492 / 0.4492\nTrain | D loss=1.376209 | G loss:0.799911 | D(x):0.449415 | D(G(z)) = 0.4494 / 0.4494\nTrain | D loss=1.376244 | G loss:0.799626 | D(x):0.449519 | D(G(z)) = 0.4495 / 0.4495\nTrain | epoch=009 | D loss=1555.038949 | G loss:904.425842\nValidation | D loss=1.376276 | G loss:0.799615 | D(x):0.449503 | D(G(z)) = 0.4495 / 0.4495\nValidation | D loss=1.376263 | G loss:0.799622 | D(x):0.449508 | D(G(z)) = 0.4495 / 0.4495\nValidation | D loss=1.376344 | G loss:0.799580 | D(x):0.449482 | D(G(z)) = 0.4495 / 0.4495\nValidation | D loss=1.376359 | G loss:0.799638 | D(x):0.449448 | D(G(z)) = 0.4495 / 0.4495\nValidation | D loss=1.376264 | G loss:0.799605 | D(x):0.449514 | D(G(z)) = 0.4495 / 0.4495\nValidation | D loss=1.376418 | G loss:0.799606 | D(x):0.449430 | D(G(z)) = 0.4495 / 0.4495\nValidation | D loss=1.376259 | G loss:0.799607 | D(x):0.449517 | D(G(z)) = 0.4495 / 0.4495\nValidation | D loss=1.376165 | G loss:0.799625 | D(x):0.449560 | D(G(z)) = 0.4495 / 0.4495\nValidation | D loss=1.376270 | G loss:0.799594 | D(x):0.449516 | D(G(z)) = 0.4495 / 0.4495\nValidation | D loss=1.376306 | G loss:0.799622 | D(x):0.449484 | D(G(z)) = 0.4495 / 0.4495\nValidation | D loss=1.376317 | G loss:0.799586 | D(x):0.449494 | D(G(z)) = 0.4495 / 0.4495\nValidation | epoch=009 | D loss=1553.863421 | G loss:902.746460\nTrain | D loss=1.376391 | G loss:0.799374 | D(x):0.449549 | D(G(z)) = 0.4496 / 0.4496\nTrain | D loss=1.376428 | G loss:0.799380 | D(x):0.449533 | D(G(z)) = 0.4496 / 0.4496\nTrain | D loss=1.376518 | G loss:0.799592 | D(x):0.449360 | D(G(z)) = 0.4495 / 0.4495\nTrain | D loss=1.376417 | G loss:0.802222 | D(x):0.448257 | D(G(z)) = 0.4483 / 0.4483\nTrain | D loss=1.376496 | G loss:0.800152 | D(x):0.449133 | D(G(z)) = 0.4493 / 0.4493\nTrain | D loss=1.376447 | G loss:0.799420 | D(x):0.449496 | D(G(z)) = 0.4496 / 0.4496\nTrain | D loss=1.376336 | G loss:0.798932 | D(x):0.449776 | D(G(z)) = 0.4498 / 0.4498\nTrain | D loss=1.376310 | G loss:0.798434 | D(x):0.450011 | D(G(z)) = 0.4500 / 0.4500\nTrain | D loss=1.376284 | G loss:0.798375 | D(x):0.450061 | D(G(z)) = 0.4501 / 0.4501\nTrain | D loss=1.376246 | G loss:0.798657 | D(x):0.449960 | D(G(z)) = 0.4499 / 0.4499\nTrain | D loss=1.376293 | G loss:0.798772 | D(x):0.449877 | D(G(z)) = 0.4499 / 0.4499\nTrain | epoch=010 | D loss=1553.922159 | G loss:902.509338\nValidation | D loss=1.376281 | G loss:0.798771 | D(x):0.449880 | D(G(z)) = 0.4499 / 0.4499\nValidation | D loss=1.376297 | G loss:0.798757 | D(x):0.449877 | D(G(z)) = 0.4499 / 0.4499\nValidation | D loss=1.376224 | G loss:0.798767 | D(x):0.449913 | D(G(z)) = 0.4499 / 0.4499\nValidation | D loss=1.376276 | G loss:0.798774 | D(x):0.449881 | D(G(z)) = 0.4499 / 0.4499\nValidation | D loss=1.376250 | G loss:0.798764 | D(x):0.449900 | D(G(z)) = 0.4499 / 0.4499\nValidation | D loss=1.376282 | G loss:0.798771 | D(x):0.449879 | D(G(z)) = 0.4499 / 0.4499\nValidation | D loss=1.376288 | G loss:0.798765 | D(x):0.449879 | D(G(z)) = 0.4499 / 0.4499\nValidation | D loss=1.376283 | G loss:0.798768 | D(x):0.449880 | D(G(z)) = 0.4499 / 0.4499\nValidation | D loss=1.376262 | G loss:0.798763 | D(x):0.449894 | D(G(z)) = 0.4499 / 0.4499\nValidation | D loss=1.376249 | G loss:0.798765 | D(x):0.449900 | D(G(z)) = 0.4499 / 0.4499\nValidation | D loss=1.376272 | G loss:0.798777 | D(x):0.449882 | D(G(z)) = 0.4499 / 0.4499\nValidation | epoch=010 | D loss=1553.818233 | G loss:901.808655\nTrain | D loss=1.376320 | G loss:0.798824 | D(x):0.449826 | D(G(z)) = 0.4498 / 0.4499\nTrain | D loss=1.376281 | G loss:0.798838 | D(x):0.449851 | D(G(z)) = 0.4499 / 0.4499\nTrain | D loss=1.376269 | G loss:0.798815 | D(x):0.449864 | D(G(z)) = 0.4499 / 0.4499\nTrain | D loss=1.376253 | G loss:0.798859 | D(x):0.449850 | D(G(z)) = 0.4498 / 0.4498\nTrain | D loss=1.376225 | G loss:0.798738 | D(x):0.449927 | D(G(z)) = 0.4499 / 0.4499\nTrain | D loss=1.415800 | G loss:0.758371 | D(x):0.460227 | D(G(z)) = 0.4806 / 0.4685\nTrain | D loss=1.376247 | G loss:0.800100 | D(x):0.449283 | D(G(z)) = 0.4493 / 0.4493\nTrain | D loss=1.376278 | G loss:0.798639 | D(x):0.449939 | D(G(z)) = 0.4499 / 0.4499\nTrain | D loss=1.376291 | G loss:0.798631 | D(x):0.449937 | D(G(z)) = 0.4499 / 0.4499\nTrain | D loss=1.376270 | G loss:0.798663 | D(x):0.449935 | D(G(z)) = 0.4499 / 0.4499\nTrain | D loss=1.376315 | G loss:0.798632 | D(x):0.449923 | D(G(z)) = 0.4499 / 0.4499\nTrain | epoch=011 | D loss=1553.815489 | G loss:902.078857\nValidation | D loss=1.376310 | G loss:0.798610 | D(x):0.449936 | D(G(z)) = 0.4500 / 0.4500\nValidation | D loss=1.376249 | G loss:0.798607 | D(x):0.449971 | D(G(z)) = 0.4500 / 0.4500\nValidation | D loss=1.376310 | G loss:0.798608 | D(x):0.449937 | D(G(z)) = 0.4500 / 0.4500\nValidation | D loss=1.376284 | G loss:0.798633 | D(x):0.449940 | D(G(z)) = 0.4499 / 0.4499\nValidation | D loss=1.376312 | G loss:0.798606 | D(x):0.449937 | D(G(z)) = 0.4500 / 0.4500\nValidation | D loss=1.376297 | G loss:0.798607 | D(x):0.449945 | D(G(z)) = 0.4500 / 0.4500\nValidation | D loss=1.376240 | G loss:0.798613 | D(x):0.449974 | D(G(z)) = 0.4500 / 0.4500\nValidation | D loss=1.376280 | G loss:0.798619 | D(x):0.449948 | D(G(z)) = 0.4499 / 0.4499\nValidation | D loss=1.376315 | G loss:0.798602 | D(x):0.449937 | D(G(z)) = 0.4500 / 0.4500\nValidation | D loss=1.376266 | G loss:0.798635 | D(x):0.449949 | D(G(z)) = 0.4499 / 0.4499\nValidation | D loss=1.376286 | G loss:0.798621 | D(x):0.449944 | D(G(z)) = 0.4499 / 0.4499\nValidation | epoch=011 | D loss=1553.818866 | G loss:901.646362\nTrain | D loss=1.376310 | G loss:0.798574 | D(x):0.449952 | D(G(z)) = 0.4500 / 0.4500\nTrain | D loss=1.376224 | G loss:0.798553 | D(x):0.450009 | D(G(z)) = 0.4500 / 0.4500\nTrain | D loss=1.376226 | G loss:0.798519 | D(x):0.450023 | D(G(z)) = 0.4500 / 0.4500\nTrain | D loss=1.376252 | G loss:0.798505 | D(x):0.450015 | D(G(z)) = 0.4500 / 0.4500\nTrain | D loss=1.375900 | G loss:0.798499 | D(x):0.450216 | D(G(z)) = 0.4500 / 0.4500\nTrain | D loss=1.376245 | G loss:0.798518 | D(x):0.450013 | D(G(z)) = 0.4500 / 0.4500\nTrain | D loss=1.376250 | G loss:0.798511 | D(x):0.450013 | D(G(z)) = 0.4500 / 0.4500\nTrain | D loss=1.376284 | G loss:0.798570 | D(x):0.449967 | D(G(z)) = 0.4500 / 0.4500\nTrain | D loss=1.376276 | G loss:0.798518 | D(x):0.449996 | D(G(z)) = 0.4500 / 0.4500\nTrain | D loss=1.376276 | G loss:0.798594 | D(x):0.449962 | D(G(z)) = 0.4500 / 0.4500\nTrain | D loss=1.376267 | G loss:0.798491 | D(x):0.450013 | D(G(z)) = 0.4500 / 0.4500\nTrain | epoch=012 | D loss=1553.791513 | G loss:901.547363\nValidation | D loss=1.376276 | G loss:0.798488 | D(x):0.450009 | D(G(z)) = 0.4500 / 0.4500\nValidation | D loss=1.376269 | G loss:0.798497 | D(x):0.450010 | D(G(z)) = 0.4500 / 0.4500\nValidation | D loss=1.376278 | G loss:0.798490 | D(x):0.450008 | D(G(z)) = 0.4500 / 0.4500\nValidation | D loss=1.376276 | G loss:0.798488 | D(x):0.450010 | D(G(z)) = 0.4500 / 0.4500\nValidation | D loss=1.376269 | G loss:0.798492 | D(x):0.450012 | D(G(z)) = 0.4500 / 0.4500\nValidation | D loss=1.376275 | G loss:0.798489 | D(x):0.450010 | D(G(z)) = 0.4500 / 0.4500\nValidation | D loss=1.376275 | G loss:0.798488 | D(x):0.450010 | D(G(z)) = 0.4500 / 0.4500\nValidation | D loss=1.376285 | G loss:0.798489 | D(x):0.450005 | D(G(z)) = 0.4500 / 0.4500\nValidation | D loss=1.376276 | G loss:0.798488 | D(x):0.450010 | D(G(z)) = 0.4500 / 0.4500\nValidation | D loss=1.376276 | G loss:0.798489 | D(x):0.450010 | D(G(z)) = 0.4500 / 0.4500\nValidation | D loss=1.376273 | G loss:0.798492 | D(x):0.450010 | D(G(z)) = 0.4500 / 0.4500\nValidation | epoch=012 | D loss=1553.818151 | G loss:901.488647\nTrain | D loss=1.376278 | G loss:0.798518 | D(x):0.449995 | D(G(z)) = 0.4500 / 0.4500\nTrain | D loss=1.376276 | G loss:0.798506 | D(x):0.450001 | D(G(z)) = 0.4500 / 0.4500\nTrain | D loss=1.376279 | G loss:0.798517 | D(x):0.449996 | D(G(z)) = 0.4500 / 0.4500\nTrain | D loss=1.376277 | G loss:0.798516 | D(x):0.449996 | D(G(z)) = 0.4500 / 0.4500\nTrain | D loss=1.376277 | G loss:0.798516 | D(x):0.449996 | D(G(z)) = 0.4500 / 0.4500\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-405424a8403c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizerD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizerG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_criterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_criterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_criterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_criterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-4ecf743c3e71>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(discriminator, generator, optimizerD, optimizerG, d_criterion, g_criterion, data, batch_size, epoch, device)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0mD_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD_logits_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0md_loss_fake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_criterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD_logits_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-12608872dd00>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mz1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtr1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# z = [b, 512, 4, 4], z1 = [b, 384, 8, 8], z2 [b, 256, 8, 8] -> [b, 640, 8, 8]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mz2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtr2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mz3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtr3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mz4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtr4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   1668\u001b[0m     return torch.batch_norm(\n\u001b[0;32m   1669\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1670\u001b[1;33m         \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1671\u001b[0m     )\n\u001b[0;32m   1672\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "d = Discriminator().to(device)\n",
    "g = Generator().to(device)\n",
    "\n",
    "d.apply(weights_init)\n",
    "g.apply(weights_init)\n",
    "\n",
    "optimizerD = torch.optim.Adam(d.parameters(), lr=0.0004, betas=(0.5, 0.999))\n",
    "optimizerG = torch.optim.Adam(g.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
    "\n",
    "d_criterion = nn.BCEWithLogitsLoss()\n",
    "g_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(d, g, optimizerD, optimizerG, d_criterion, g_criterion, train_data, batch_size=64, epoch=epoch, device=device)\n",
    "    val_loss = validate(d, g, d_criterion, g_criterion, train_data, batch_size=64, epoch=epoch, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(g.state_dict(), 'Generator_normal_10epochs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python37564bitf9fd249920114b9f9eb7d753607eb13b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
